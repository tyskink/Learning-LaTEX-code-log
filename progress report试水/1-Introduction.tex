\chapter{Introduction}
\section{Problem and Motivation}

\par Recently years, the Artificial intelligence (AI) has been developed quickly especially since the AlphaGo \citep{silver2016mastering} in 2016. Canonical processor has shown its weakness to produce this kind of massively matrix addition and production operation. GPU, FPGA and many other ASIC AI chip has shown their performance in this domain. It seems like the canonical processor will not able to be developed furthermore. Since 2004, the single core processor met the failed of Dennard-scaling and then turned from single-core to multi-core \citep{hennessy2017computer}. The increasing rate of processor performance has slowed down year by year\citep{hennessy2017computer}. The power-wall, memory-wall, and even the clock-synchronization has become the bottle-neck of processor performance. Canonical Processor are still occupied the most domain in computing, the slowdown of its performance has limited many domains of society. To combat these problem, we need pay more attention in memory access, energy saving techs and so on.

\par Massively Parallel Processors (MPPs) is a kind of techs to combine thousands of processors together to produce a huge computation problem. Many-core has shown its performance in energy management\cite{borkar2007thousand}, such as big.LITTLE architecture of ARM\cite{padoin2014performance}. SpiNNaker Project\cite{furber2013overview}\cite{furber2014spinnaker} has develop several different scales of event-driven MPPs for brain simulation based on Spiking Neural Network. This kind of architecture could also be used for another domain such as weather simulation and physical simulation. Compare with ASIC chips, this kind of architecture will be more universal. Compare with FPGA, this kind of architecture can be powerful with faster clock speed. Compare with GPU, this kind of architecture can be more energy efficiency. Above all, the aim of MPPs is using simple, cheap and self-organized cores to achieve a desktop-level supercomputer, fill up the black domain between general purpose processor and application specific chips. 

\subsection{The Trend of AI}
\par The algorithm of AI, nowadays the most popular method is deep learning, which combine a massively of matrix computation as a network. Though the work of compression of AI algorithm is still continuing, the mainly computation of AI is still in the scope of matrix processing, or sometimes tensor processing. This kind of computation has taken a higher request for processors. 
\par 
At the beginning of the application of AI algorithm, until 1st generation AlphaGo\cite{silver2016mastering} in 2016, the computation of AI algorithm will always in CPU and other supercomputers. With the successfully of AlphaGo and the development of ImageNet Challenge\cite{dean2012large}, especially the help of NVIDA’s CUDA, most of the work could be down more efficiently in GPU\cite{ren2015faster}\cite{schmidhuber2015deep}.Many AI chips were also developed for this computation, such like TPU of Google\cite{jouppi2017datacenter}\cite{silver2017mastering} and DaoDianNao chip\cite{chen2014dadiannao}. FPGA also take a place of AI computation\cite{schmidhuber2015deep}\cite{zhang2015optimizing}. Xilinx and Altera also developed specific library for AI algorithm. Neuromorphic processor such like TrueNorth\cite{merolla2014million} using analogy circuit and SpiNNaker\cite{furber2014spinnaker} based on digital circuit with off-the-shelf core.
It is hard to say which one is the best, they all have their own advantages and disadvantages with different application and environment. But the challenges are still related with the challenge of the canonical processor: memory, energy performance and price.

\subsection{The Ceiling of Multi-core}
The limitation of Thermal Dissipation(TP) has lead to the dark silicon\cite{esmaeilzadeh2011dark} problem\cite{taylor2012dark}. The cores in a chip could not able work in the same time, and Esmaeilzadeh indicated this might be one reason of the end of multi-core if no driver occur\cite{emer2002asim}. Whether this statement, TP problem has actually limited the heap performance of one chip and reduce the performance-price ratio.
\par 
Another problem is latency, this latency mainly appears in processor-memory communication\cite{hennessy2017computer}. The gate switching speed is limited by voltage. And the communication speed is limited by light speed. Though the Moore’s Law is still working, the nanometre techs has actually slow down and wire delay scales is much slower than transistor performance\cite{hennessy2017computer}.
\par 
With the development of nanometre techs, the die size has become smaller year by year. The size of a chip and number of cores will pay less in the price cost of one chip. Dark silicon limits the number working core in one time, but ARM’s big.LITTLE architecture limits the working core themselves to saving energy. Eventually, the SpiNNaker chips, using event-driven mechanism, has much larger dark silicon than canonical processors. This shows that, dark silicon is a result of TP limitation, but could also be a method of saving energy. In this aspect, multi-core has much more bright future.

\subsection{Programming Problem}
With the increasing of number of cores, the thread management and task distribution might one complex problem. If all cores produce different thread, or with coarse granularity parallelism, the programming is not a problem. But such like matrix computation, it need fine granularity parallelism and need highly synchronization. Synchronization problem involves the memory, cache coherency, communication and many other question, which makes the system complex. SpiNNaker uses Global Asynchronization Local Synchronization (GALA) and with Spiking Neural Network is an excellent solution as a MPPs. But Not all application is suitable to use Spiking Neural Network, and how to organize the architecture of MPPs will affect the choice of programming model. For some problem, such as memory coherency, can be solved with either hardware or software, how to balance the work of hardware and software is another problem.
\par 
Parallel programming, or concurrent programming is still a complex work. However, if MPPs could use or transfer mathematical model directly to program, it might be much easy than single core programming. 

\section{Research Challenges}
Using MPPs for science simulation, AI algorithm and other kind of application, make a more universal MPPs is a new and unpopular work. For me, my fundamental knowledge is not enough, and has less practice in progress design is the main problem.
\par 
For this research, firstly, design work is a huge challenge, how to finish the design using more efficient tools, language and with more computer assistant is one problem. Simulation is another problem, MPPs has many cores and the workload of simulation is heavy. How to evaluate the work and how to develop the application is a harder work. 

\section{Research Objective}
With this research, one kind of MPPs architecture could be generated hopefully. But more important contribution is the method for design a MPPs. In this progress, inter-chip and intra-chip communication will be researched and a new and effective communication protocol might be designed. Simulation work will be another important contribution. With modern high-level synthesis language such like SystemC, many platforms could be used for the simulation of hardware. An idea of how to balance programming and hardware workload could be down and the conception of programming on MPPs could be more popular.

\section{Report Structure}
For this report, a literature will be put firstly, involves the basic knowledge of processors and parallelism. The simulation and computer-aided design method and tools will be review. Trend of hardware and architecture design will be included. GPU and SpiNNaker will be introduced as case study. Finally, an incomplete SystemC RISC-V model is the result of this period. 
