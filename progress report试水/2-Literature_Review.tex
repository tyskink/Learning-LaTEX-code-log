\chapter{Literature Review}

In this Chapter, a review of the computer architecture will be given, to show my basic knowledge in this domain. Based on the book \textit{Computer Architecture: A quantitative Approach 6th edition}\cite{hennessy2017computer} and \textit{Computer Organization and Design: The Hardware/Software Interface - RISC-V Edition}\cite{patterson2017computer}. The fundamental knowledge of computer architecture is prepared for the RISC-V practice. The section of parallel architecture, communication and network interconnection are prepared for the MPPs and NoC design. The above literature review will introduce the basic terminologies and more focused on the new trend. Finally, the simulation part is the literature preparation for the work of the first year.

\section{Fundamental of Computer Architecture}
In 2015, the International Technology Roadmap for Semiconductors (ITRS Report) had stopped at the February Workshop\cite{ceze2016arch2030}\cite{ITRS2015}. Nowadays, the Dennard scaling has actually failed\cite{hennessy2017computer}, and Moore's Law may also come to its end in several years\cite{chien2013moore}\cite{courtland2016transistors}\cite{national2011future}.  Meanwhile, since the Von Neumann architecture\cite{von1993first} been presented in 1945, there is no momentous change in the computer architecture. And until now, the development of architecture is still limited by the Semiconductor manufacturing processes.

\subsection{Instruction Level Parallelism (ILP) Wall, Power Wall, and Memory Wall\cite{wulf1995hitting}}
%TDP problem ->power wall: from clock frequency to multi-cores
%??? -> limits the ILP increasing: from ILP to TLP, DLP
%access speed -> memory wall: adding caches and advanced NoC
%should rewrite this section!!!
Single-core performance is limited by many factors, the ILP Wall and Power Wall are the two most important\cite{hennessy2017computer}. ILP techs has been developed much in last century. Instruction pipeline, super-scalar execution, out-of-order execution, register renaming, speculative execution and branch prediction all help significantly on the improvement of single-core performance. However, many limitations has been found while the people trying to continue increasing the ILP ability\cite{postiff1999limits}\cite{Wall1991176Limits}. And due to the inefficient in silicon and power utilization while increasing the width of issue, the task of improve the performance has fall onto the increasing number of caches and cores per die\cite{hennessy2017computer}.
\par 
Dynamic scheduling, speculation and multi issue are three most important ideas of ILP, but after the Intel's Itanium\textsuperscript{\textregistered}, the research of speculation and multi-issue in ILP level has stopped\cite{winkel2004exploring} and researches about ILP may always combine it with Thread Level Parallelism together\cite{wang2009dynamic}\cite{xekalakis2012mixed}. 
\par 
Memory Wall is also not a new term, after the first indication by Wulf in 1995\cite{wulf1995hitting}, which is always the most serious problem in processor design\cite{asanovic2006landscape}.
\subsection{Instruction Set Architecture (ISA)}
There are many ISAs such as x86, MIPS, ARM, RISC-V et.al. ARM and x86 has occupied most of the market. 
\subsection{Multi-core}
Asanovic listed 12 reasons about the turn from single-core to parallel computing in 2006\cite{asanovic2006landscape}, which could be concluded to four factors: power (power wall and static power), communication problem (increasing bandwidth-latency gap, memory wall), transistor level and manufacturing problem (soft and hard error rates and reliability, wire delay, noise, cross coupling of capacitive and inductive, clock jitter) , the increasing design complexity and the end of the research in ILP (ILP wall, manufacturing variability, design cost). The Berkeley Report called which is a 'brick wall'\cite{asanovic2006landscape}.
\par According to Hennessy\cite{hennessy2017computer}, today's researches, based on multi-core, has met some other problems. Memory, or in terms of the communication of processor-memory and processor-processor. The processors which have many modules usually use network on chips, router and mesh topology rather than simple shared bus to help the communication. Message passing, and shared memory are two main method to exchange data packet. However, message passing request a high requirement to the router and shared memory has limited by the memory latency. The Amdahl's Law and the end of the Dennard Scaling also limit the development of multi-core.
\par Dark silicon or power wall, and memory wall are still the bottleneck of classical Von Neumann architecture. After a crazy increasing of the number of cores during 2014 with the boom of personal mobile device (PMD), eventually to 16 cores and more, the number of cores has come back to 8 or 4.
\subsection{Heterogeneous and the future of computer}
\par Start with the development of personal multimedia and the demand of image processing, GPU has start taking an important in computer as CPU. And with the demand of artificial intelligence, the will be more modules are under the need to be developed to support more function. Domain-specific and heterogenous are two important trends\cite{chung2010single}\cite{hennessy2017computer}\cite{taylor2012dark}. 
\par However, while we review the road of the computer, sometimes we can find out that there are many ideas could be found in the papers which published ten or twenty years ago. Such as the relationship between vector processor and GPU, such as the reawakening of In-Memory Computing (IMC) and Processor-in-memory (PIM). They were just limited by the nanometer technologies or materials before and can survive due to the improvement in semiconductor manufacturing processes. 
\par In the history of computer, there are always two physical barriers has in the road of processor: silicon and light speed. The decreasing size of die has led to the popular of Internet of Things (IoT) device, the demand of the computation of IoT devices has led to the research trend of edge computing. The research of Big data could only base on the data collected from IoT devices and PMDs. Cloud computing is actually providing service for the computation of big data and mobile devices. Maybe we can say that, the improvement in computer architecture is not the contribution of computer architecture designer but the contribution of micro-electronics engineers. 
\subsection{Non-silicon and the effect of new materials}
\par Mixed-signal IC is introducing analogue signal back to modern digital circuit. TrueNorth\cite{merolla2014million} is one of the successful achievement of analogues circuit. At the critical time for computer - high demand of artificial intelligence and the end of silicon manufactory - many new opportunities have occurred. 
\par Carbon-nanotubes computer\cite{kreupl2013electronics} or carbon-based circuit might be the future. Quantum computer might be another road. Biopolymer and the introduce of superconductivity\cite{manheimer2015cryogenic} may also give a chance to new computer. 
\par The techs above look like a dream, but some new materials may improve the situation in the near future. Three demission design of IC and the use of new material on memory - such as the Phase-changed RAM\cite{hatayama2018inverse} and Resistive random-access memory (ReRAM)\cite{akinaga2010resistive}. Which may lead to several trends in computer architecture: NoC from 2D to 3D, topology from mesh to cube, larger and quicker caches, improvement in energy efficiency. Memory management and data communication are the two factors may be affected. 
\subsection{Summary}
\par Physical material controls the death or rebirth of the design of computer. However, we could do more with the existing materials. The trend now is from ILP and TLP to (Data Level Parallelism) DLP and Request Level Parallelism (RLP)\cite{hennessy2017computer}. Reconfigurable architecture and heterogeneous many-core are two import forms. Application specific or domain specific is the most important guide line while designing computer architecture. Energy efficient and performance are the only two target the designer need to catch.
\par It is no doubt that the physical level thing has limited the processor too much, either making architecture design more independent or helping them two more related could all help the design work reducing complex. However, it seems like the processor design is closer to software level design, FPGA is now helping the hardware and software design mixing together. The revival of neuromorphic chips is also making the boundary between software and hardware much vaguer. In the future, the design work in ILP, TLP, DLP and RLP may be intermingled, with the help of the computer-aided design and the development of artificial intelligence. Hopefully, the design progress of a domain specific chip or application specific circuit will be as simple as software design.

\section{Parallel Architecture and Topology}
\par The research in parallel architecture is usually in multi-core and multi-processor. Multi-processor has been the mainstream architecture at least after 2005, and the range of whose conception has also increased in these years. In Hennessy's \textit{computer architecture}\cite{hennessy2011computer} written in 2011, the number of processors of a multi-processor is "from a dual processor to dozens of processors", and in the newest edition of the same book written in 2017\cite{hennessy2017computer}, "sometime hundreds of processors" has been added to the range in size. The focus of multiprocessor in this book also changed from 2~32 cores in 2011 to 4~256 cores in 2017.
\par For the parallel Architecture, the book \textit{Parallel Computer Architecture: a hardware/software approach}\cite{culler1999parallel} written by DE Culler in 1999 is one of the basic textbooks. Which introduces the parallel computing from performance and evaluation, shared memory multi-processor, snoopy protocol and directory protocol to scalable multi-processor, interconnect network and latency tolerance to hardware and software trade-off. To today's view, the only useful part of this book is the shared memory multiprocessor chapter. Which is a part of the multi-processor trend in the following ten years.
\par The most popular parallel architecture are multi-core and GPU, the former execution program in thread level and the latter execution program in instruction or data level. Meanwhile, FPGA could be another kind of parallel architecture which combined multiple Look-Up-Table (LUT) working together in parallel. If we extend the concept of parallel, ASIC or all digital circuit could be a parallel computing architecture which making all transistors working in parallel. According to Hennessy\cite{hennessy2017computer}, there are several levels of parallelism: ILP, TLP, DLP and RLP. And which could be two basic classification in application: data level parallelism(DLP) and task level parallelism (TLP). And Flynn, according to the parallelism in instruction and data, made four classes for computer architecture: SISD, SIMD, MISD, MIMD\cite{flynn1966very}. And then, more different kinds of parallel computer appeared: Parallel vector Processor (PVP), symmetric multiprocessing (SMP), massively parallel processing (MPP), Cluster of Workstations (COW), Non-Uniform Memory Access (NUMA), Cache Coherent Non-Uniform Memory Access (CC-NUMA), Distribute Shared Memory (DSM). All these conceptions were mentioned at last century and some of them went death, and some of them rebirthed again.
\par For this section, the basic concept in parallel computation will be reviewed, the challenges and the trend of parallel computing will be discussed. 
\subsection{Definition and organization of parallel architecture}
\par Levels of Parallelism
\par According to Hennessy\cite{hennessy2017computer}, and some other literatures, there will be several different levels of parallelism:
\par Bit Level: Implicit level, refers to how many bits could be processed in the traditional computer, such as 8, 16, 32, 64bits. Usually indicates the bus width.
\par Instruction Level: Implicit level, is the mainstream aspect while the researchers focused on the single-core performance. Aimed to increasing the number of instruction to be processed per clock cycle or per second.  
\par Data Level: Explicit level, aimed to process more data in one time. GPU, Vector Processor and Instruction Set SIMD Extension (such as AVX for X86 and NEON for ARM) are three important expression to this level of parallelism. And the research has start transforming from hardware to software from this level.
\par Thread Level: Explicit Level, for improving the utilization of single-core, by adding more program counter. For these years, TLP often appears with multi-core and multi-processor, 
\par Task Level: Explicit Level, sometimes is the alternative name of Thread Level. Which is composed by function or task in application view.
\par Request Level: Explicit Level. And aims to response the requests from customers with several decoupled tasks. 
\par Depth/Gain of Parallelism:
\par Totally three kinds of grain: fine, coarse and embarrassing. It indicates the communication frequency between different thread. In the other hand, it also refers to in which level of parallelism the most important communications happen.
\par Types of Parallel architecture 
\par According to the number of processors or cores, the parallelism could happen on uniprocessor, multi-core, and multi-processors. The parallelism on uniprocessor are scalar processor, vector processor and the processors using SIMD instruction set. Then have just one processor but could issue multi-instructions and execute instruction level parallelism. The multi-core system has multiple cores or execution unit. They could execute parallel tasks in data level (GPU) or thread level (multi-core) on one chip. Multi-processor system includes cluster and massively parallel processors. For this part, we will focus on the multi-processor and multi-core architecture.
\par According to the programming model, the parallel architecture could be shared address or message passing. Which could also be called the communication models, communication mechanisms or data exchange forms between parallel tasks. There are many researches have compared these two kinds of architectures\cite{hennessy2017computer}\cite{klaiber1994comparison}\cite{leblanc1992shared}. And in fact, there are many terms or subclass for parallel programming model, such as multiprogramming, shared address, message passing and data parallel. In another view, which could also be divided to shared address space architecture or distributed address space for all large-scale multiprocessors\cite{hennessy2017computer}. See Figure 1. Thread model is one important kind of shared memory programming, but for programmer, they will more focus on the thread level parallelism. For example, the POSIX Thread, OpenMP and the thread developed and used by Microsoft, Java, Python and CUDA. What's more, the Data Parallel Model will also be regarded as one important class of programming model, which may also be referred to as the Partitioned Global Address Space (PGAS) model.
 
\par Figure 1 The space of large-scale multiprocessors and the relation of different classes\cite{hennessy2017computer}
\par Shared Address Space/ Shared Memory Architecture
\par The foundation to classical Shared Memory Multi-Processor (so called shared address space) is the organization of memories and interconnect strategy\cite{hennessy2017computer}, in terms of how the modules in computer (mainly indicates the cores) exchange data and messages with each other. According to the organization method of memory, Shared Memory Multi-Processor has two basic class: Symmetric Multi-Processors (SMP) or Uniform Memory Access (UMA) using one centralized memory and Distributed Shared Memory (DSM) or Non-Uniform Memory Access using distributed. The former one is usually used in small scale multi-processor system, but the mainstream number of cores has increased from 8 in 2011 to 32 in 2017\cite{hennessy2011computer}\cite{hennessy2017computer}. The later one could help increasing the ceiling of bandwidth and latency with a more complex software support to increasing the utilization of existing bandwidth. It could achieve better performance in large scale multi-processor systems, and in these years, more and more small scale multi-processors have start to use this architecture and most of the multi-chip systems are using this architecture\cite{hennessy2017computer}. All the shared memory processors using shard memory communication rather than message passing, and one of the limitation of shared memory multi-processors is the cost of communication\cite{hennessy2017computer}. As another limitation, the lack of parallelism of traditional program has limited the speed up according to Amdahl's Law.
\par Distributed Address Space / Message Passing Architecture 
\par For distributed address space, which means that there will be many processors with private memory space. In most of the system using distributed address space, message passing is used as the communication model. For the other terms, such as Massively Parallel Processors and cluster, the meaning of them still doesn't have an exact range\cite{dongarra2005high}, but we could use large-scale multiprocessor to conclude them\cite{hennessy2017computer}. 
\par The target for this project is focusing on Massively Parallel Processors (MPPs). The MPP is one kind of large scale multiprocessor architectures, built by multiple small processors and always used for scientific applications.\cite{hennessy2017computer} The term "massive" will also changes with the time. General, the Illiac IV in 1970s is regarded as the first MPPs with a SIMD array consisted of 64 processing units and connected by a 2D torus with each other\cite{hennessy2017computer}. And then is the cosmic cube\cite{seitz1985cosmic} with 64 processors in 1985. The connection machine\cite{Hillis1985The}\cite{leiserson1992network} using 64K 1bit processors is one successful implementation of MPPs in 1980s. The idea of using on bit processor to build a thinking machine still have an important meaning for today's AI chips. The research on MPPs then turn to high performance computing with low dimensional topologies in 1990s, such as the "Intel Paragon, Cray T3D, Cray T3E, HP AlphaServer, Intel ASCI Red, and IBM Blue Gene/L"\cite{hennessy2017computer}. However, due to the cost of MPPs, the supercomputer market had been occupied by cluster\cite{hennessy2017computer}\cite{top5002017supercomputer}.
 
\par Figure 2 Architecture of Supercomputers Nov 2017\cite{top5002017supercomputer}
\par The though the massively parallel processors cost more than cluster, the performance of which is better. For example, the Sunway TaihuLight using MPP array and RISC instruction set. Meanwhile, the SpiNNaker\cite{furber2013overview}\cite{furber2014spinnaker} using massively parallel processor also provided the idea: using MPP as a neuromorphic system to run a neuro network model for massive parallel processing. 
\subsection{Synchronization problem}
\par For parallel processing, the most important is the communication between threads, or in terms of the execution units. The communication models for massively parallel processors are message passing and shard memory, the former one will be introduced in 2.3, and the later one will be introduced in 2.4. 
\par For the system using global clock, the synchronization of clock will be one serious issue, especially for the systems like MPPs who use large scale board. Meanwhile, with the increasing area of on chip or the increasing size of one system, the clock problem in communication, including clock skew and jitter, wire latency, should all be considered. Synchronization or asynchronization are two ways to achieve one network. Some other design also uses global asynchronization local synchronization (GALS) design\cite{furber2013overview}\cite{hemani1999lowering}. 
\par For the practice of parallel systems, data dependencies will affect the synchronization design. The Bernstein's Condition raises the requirement for synchronization. The method for synchronization could be hardware and software. For Hardware Support, Test-and-Set is one primitive method, Load Link/Store Condition will always be used for MIPS, ARM, PowerPC and Alpha, and Compare-and-Swap (CAS), Test-and-Set, Atomic Increment, compare-and-exchange and Bus Lock Prefix will usually be supported by x86.  Software synchronization like POSIX Thread support locks, barriers and condition variables, and also support a high-level method like the monitors, parallel sections and loops.
\par Atomic operation will always be used for concurrent processes. For example, the test-and-set for the synchronization achieving while accessing critical on uniprocessor.
\subsection{Performance overhead and speedup} 
\par Amdahl's law and Gustafson's law have indicated one serious problem, that is the speedup of a parallel system is limited by the proportion of parallelism part of the program. Amdahl's Law shows that there will be a diminishing return in the performance with the increasing number of cores, if the parallelism of program has no change\cite{hill2008amdahl}. 
\par For the meantime, the Operational Intensity, which indicates the transferred operations rate between execution unit and DRAM, has affect the performance of multi-core system\cite{williams2009roofline}. The transferred rate is limited by processor-memory communication speed. That is the ubiquitous memory wall.
\par However, manycore as FPGA, are using cheap, simple and smaller cores to provide the chance. SpiNNaker has provided an idea, that is developing one system on neuro level parallelism and making neural network as the programming model. The neural network itself could provide massively parallel chance and the performance has already be confirmed by recent year's practice. Meanwhile, using the architecture of traditional massively parallel system and manycore system, to develop a self-organized, reconfigurable network to support this kind of neural network will not be an unreachable goal. The challenges are the system and software environment.
\section{Communication, Network on Chip (NoC) and System on Chip (SoC)}
\par As the communication architecture, shared memory communication and message passing are two basic idea. Using shared memory with global variables for exchanging data will always use in small scale machine. Message passing communications use router or other message controllers to forward data between terminals just like a small Internet. 
\par For the shared memory communication, the bottleneck is now focusing on the access speed to memory and the memory consistency. For the message passing communication, though the memory access bottleneck should be considered, the topologies, routing method and algorithms have become the important parts in this kind of traffic and finally were grouped together as the Network on Chip technologies. The shared memory part has been down in 2.4, and for this section, the details of interconnect network will be discussed. 
\par Many books have already given the basic knowledges about NOC during the first decade years\cite{hennessy2017computer}:
\par \textit{Networks on Chip}\cite{jantsch2003networks} published in 2003 by Axel Jantsch collected 14 chapters written by different experts in three parts: system design, hardware design and software interface.
\par \textit{Interconnection networks: an engineering approach}\cite{duato2003interconnection} in 2003 is the most influential textbook for NoC. It introduced the NoC in engineering view.
\par \textit{Principles and practices of interconnection networks}\cite{dally2004principles} in2004 is regard as one of the most important classical textbooks for learning NoC. It introduces the Interconnection network (System Area) with three major parts: topology, routing and analysis and is more focus on the topology.
\par As we can see, the theory of interconnection network on chip had already become quite mature in the last 10 years. For example, the deadlock problem was introduced by Holt in 1972\cite{holt1972some}, and the avoiding techs were developed during 1980s\cite{merolla2014million}\cite{gunther1981prevention}\cite{dally1987deadlock}. The switching technologies were also developed during 1980s and 1990s, for example the pipelined switching with virtual cut-through\cite{kermani1979virtual} in 1979 and the wormhole switching\cite{dally1986torus} in 1986. And then, such as the virtual channel and escape channel were all developed and help avoiding the deadlock.
\par According to Pinkston and Duato\cite{hennessy2017computer}, the interconnection network could be grouped to four levels based on their size: 
\par On-Chip Network (OCN) or Network on Chip (NoC) refers to the network used in one chip to connect the functional units of processors. 
\par System Area Network (SAN) and Storage Area Network aims to solve the processor to processor connection and processor to memory connection problem. For example, the network for a massively parallel processor system.
\par Local Area Network (LAN) refers to the network to connect computers in room wide. Such as the Bluetooth LAN and Wi-Fi LAN.
\par Wide Area Network (WAN) refers to the long-haul network, for example, Asynchronous Transfer Mode and Internet.
\par In this section, the OCN and SAN who are useful in Massively Parallel Processors will be discussed.
\par In one network, the node, interface, switch/router and link/channel are four basic components, messages (packet and flit) will be transferred through the whole network and finally arrive the destination. During the design of this kind of network, topology, routing algorithm and flow control are three issues should be decided. The scalability, QoS, performance (throughput, latency) and energy efficiency will all be affected by these decisions.

\subsection{Topologies and architecture}
\par There are many kinds of topologies for NOC, started from the simplest shard bus (single line) and rings, meshes crossbar and finally to the hypercube in system area, researcher have almost tried all the forms could be thought and finally come back to simple topology since 1990\cite{dally1990performance}\cite{hennessy2017computer}. 
\par The basic topologies have already been introduced by many resources, bus, crossbar, ring, mesh, and torus are still the most popular designs in modern processors. Topology is the foundation of the NoC design. For one specific topology, there are several properties to evaluate if which is suitable for one design: direct/indirect, blocking/non-blocking, the cost, latency, throughput (bisection bandwidth) can be calculate according to topology.
\par As mentioned before, the clock synchronization problem, whether use global clock or not, how to implement GALS should be considered while designing the topology.
\subsection{Router, routing algorithms and flow control}
\par Routing method decides the route of one message, and flow control decides the how the storage and buffer working in this route. The routing can be Oblivious (deterministic/static or random) or adaptive (dynamic, for avoiding deadlock and livelock)\cite{dally2004principles}. With different topology design, the routing algorithm is also different. The table describe several main routing algorithms.

\par Table 1 The classic routing algorithms
	Routing method	Description and bibliographic 
Deterministic	Randomized routing	Be described by Valiant in 1981 firstly\cite{valiant1981universal}. 

	minimal oblivious routing algorithm	According to Nesson and Johnsson, 1995\cite{nesson1995romm}

	Load-Balanced Oblivious Routing	Introduced by Singh in 2002\cite{singh2002locality}

Adaptive 	wormhole routing strategy	Linder's wormhole routing strategy with virtual channel in 1991\cite{linder1991adaptive}.
A survey for wormhole routing in 1993\cite{ni1993survey}.
For mesh and without virtual channel in 2000\cite{chiu2000odd}.

	Adaptive routing with virtual channel	Based on block faults and using virtual channel in 1992 by Chien.\cite{chien1992planar}
Dally made the free virtual channel's number as the congestion metric in 1993.\cite{dally1993deadlock}\cite{ramakrishna2016gca}

	Minimal adaptive routing	Used on fat tree in CM-5 by Leiserson in 1992\cite{leiserson1992network}

	Fully Adaptive Routing	
	Load-Balanced Adaptive Routing	
	Search-Based Routing	
\par In 1988, Dally indicated that removing cyclic dependencies on channel dependency graph is one necessary and sufficient condition for designing a deadlock-free routing algorithm\cite{dally1987deadlock}\cite{ebrahimi2017ebda}. the turn model then made this theory be basic to adaptive and deterministic routing and free the restriction on wormhole networks.\cite{ebrahimi2017ebda}\cite{glass1992turn}
\par Meanwhile, Duato also showed a necessary and sufficient condition to free the restriction while using channels for designing a deadlock free fully adaptive routing.\cite{duato1993new}\cite{ebrahimi2017ebda}
\par The flow control is not only just storing data to buffer, but also usually be considered together with adaptive routing. The topics of adaptive routing are usually how to avoid the deadlock and livelock, how to improve the fault-tolerance performance, how to work on a reconfigurable network, how to keep the load balance. There are several flow control methods, such as the circuit switching, packet-based store and forward, packet-based virtual cut through and flit-based wormhole flow control.
\par To analyse the performance of one routing method, worst-case analysis method\cite{towles2002worst} could be used for oblivious routing. 
\subsection{Communication and protocols}
\par For architecture level communication model, the message passing model and shared memory communication are two classes. This part will focus on the protocol and interface in NoC view and discuss the one-to-one, one-to-many and one-to-all communication mechanism.   
\par In massively parallel processors, whether it is using direct or indirect network, the communication can be concluded to processor-processor, processor-memory, processor-switch and switch-switch communication. For message passing model, the communication will focus on the processor-processor communication or in terms of processor-router communication and router-router communication. For shared memory system, it is processor-memory communication. 
\par There are several properties for communication protocols: synchronization or asynchronization, how a clock aids the communication, how many wires the channel needed. Besides the simplest instruction-like control signal, there are still many protocols could help achieving a more complex network and communication task. 
\par Moreover, which could also be classed by the type of interface: system control interface (i.e. clock gating), standard bus interface (i.e. AMBA, OCP, SRAM, MIPI), non-standard but interface, test interface (i.e. DFT) and other miscellaneous control interface.
\par The most common practice is Inter-Processor Communication (IPC), which contents a set of methods such as I2C, SPI, and SDIO which need to connect to a slave or a client controller. UART is the simplest interface for embedded processors. These protocols are also suitable in other communications. 
\par For on-chip processor-memory communication, the DMA protocol also plays an important role. 
\subsection{QoS and reliability}
\par There are many aspects on software level and algorithm level needed to be considered while designing a NoC system. The research on topology, flow control and routing are focusing on improving the efficiency of a network in more hardware level. However, while allocation the communication tasks, there will also be a fair problem. Moreover, the arbitration focuses on dealing multi-request for limited resources. 
\par Error Control and Fault tolerance are important to keep resilience, reliability and availability.
\subsection{Furthermore, and conclusion}
\par There are many technologies which may change the form of NoC, such as the Optical NOC\cite{vantrease2011atomic}, wireless communication\cite{abadal2015broadcast} and 3D Stacking.
\par 3D stacking could help in increasing bandwidth and reducing the latency and power\cite{asanovic2006landscape}. And many researches have pay attentions to this field\cite{murali2009synthesis}. But this tech has been indicated over at least 20 years, and still not be able to achieve onto custom device. It is better to ignore its influence in at least five years. 
\section{Memory Hierarchy and Cache Coherency}
\par The performance of modern processors is limited by the communication speed, and the memory access speed is one of the important bottlenecks in the process of communication. For the best and simplest way to achieve a high-performance processor, it is better to read/write data from one large memory with high bandwidth and low latency. However, it is just a dream now due to the area limitation and material limitation and cache has become an alternative way for improving memory performance. 
\subsection{Memory Hierarchy}
\par Cache is the most important part in the memory systems for improving the performance of a processor\cite{goodman1983using}. 
\par L1 Cache (First Level Cache)
\par For the most processors, there will be an Instruction Cache (I-Cache) and a Data Cache (D-Cache) as the level one cache. These two caches separate data and instruction are organized as a Harvard Architecture and aims to increasing the access speed. The L1 Cache is mad by SDRAM which using 6 transistors for one bit\cite{hennessy2017computer}. So, the L1 cache could not be too larger, otherwise will occupied too much of area and may influence the latency. And the L1 cache also limited by the clock cycle\cite{hennessy2017computer}.
\par L2 Cache (Middle Level Cache)
\par L2 Cache aims to improve the hit rate with a larger SRAM than L1 Cache. 
\par With the L1 cache and L2 cache, the hit rate could be keep at least above 95\% in modern processors\cite{Hruska2017blog}. 
\par L3 Cache (Last Level Cache)
\par The L3 Cache appears with the multi-cores, as a shared memory for all cores to improve both the hit rate and access time. 
\par L4 Cache (eDRAM)
\par Sometimes there will be a L4 Cache as an eDRAM for the communication between CPU and GPU.
\par RAM
\par Usually using SDRAM or DDR RAM as the main algorithm memory.
\par ROM
\par Using EEPROM (such as Flash) or Hard Drive as a storage memory.
\subsection{Memory Materials}
\par For nowadays, SRAM is the fast and most expensive memory. Which is built by 6 transistors per bit to achieve high performance. And DRAM is a middle performance memory with only single transistor per bit to get a balance between cost and performance. The NOR and NAND Flash memory has recently become the mainly memory material as an alternative of ROM and DRAM. However, all the memory techs are still not good enough for the modern processor.
\par Area is one part: the L1, 2, 3 Cache has occupied half the area in most processors, with high power consumption. Access speed is another part: even using the cache and achieving a high hit rate, it still need several clock cycles to get data from memory. What's more, the access time of main RAM and ROM also need to be improved, especially for the increasing size of data due to the development of multimedia and machine learning.  
\par For recently years, there are several memories may affect the future processors.
\par NRAM\cite{nantero2018NRAM}
\par NRAM is one kind of Non-Volatile Memory (NVM) based in Carbon Nano Tubes(CNT). It may able to be the alternative of EEPROM and NOR Flash.
\par ReRAM
\par ReRAM is a kind of NVM aimed to replace Flash RAM. Which also re-leads the trend of In-memory Processing and ReRAM Computing in neural network domain\cite{chi2016prime}.
\par PCM (Phase Change Memory)
\par Is indicated as one important techs to replace the DRAM as the main memory\cite{qureshi2009scalable}.
\subsection{Memory Consistency and Cache Coherency}
\par The fundamental knowledge has already explained explicitly by Sorin in book \textit{A Primer on Memory Consistency and Cache Coherence}\cite{sorin2011primer}. 
\par Cache Coherence protocol 
\par The Cache Coherence aimed to solve the incorrect problem while reading an address and make sure the value read from cache line is the same with the present, newest one. In terms of spatial coherence. The main work were down in last century, such like the early work in hardware cache coherence\cite{goodman1983using}\cite{papamarcos1998low}\cite{wilson1987hierarchical} and the early survey about cache coherence protocol\cite{archibald1986cache} in 1986. 
\par The term Cache Coherence comes from Distributed System, and now has two main kinds of implementations or mechanisms in shared memory systems: Snoopy Coherence Protocol and Directory Coherence Protocol. To add states (Modified, Shared, Invalid, Exclusive, Owner) to these two protocols, VI(valid/invalid), MSI (Modified, Shared, Invalid), MESI (Illinois protocol: Modified, Exclusive, Shared, Invalid), MOSI (Modified, Owner, Shared, Invalid), MOESI (Modified, Owner, Exclusive, Shared, Invalid), MESIF (Modified, Exclusive, Shared, Invalid, Forward), MERSI (Modified, Exclusive, Recent, Shared, Invalid). And for different level of cache, protocol could be different. For example, the ARM implements MOESI to L1 Cache and MESI to L2 Cache\cite{ARMA57TRM}. 
\par In the traditional snooping protocol, all the cache controllers must monitor all the activities from others to maintain the state in coherence, need a bus or ring as the medium for broadcast the caches activities. Usually used with buses in small scale Chip Multi-Processor systems (such as centralized shared memory architecture\cite{hennessy2017computer}) and require a totally-ordered interconnect\cite{martin2003token}\cite{sorin2011primer}. The snooping protocol has a bandwidth overhead due to this kind of requirement of fully-interconnect\cite{zebchuk2009tagless}, and directory protocol could help this kind of ceiling by using a central control device to handle all cache activities (provide the global state or summary state) and could be implemented to distributed memory systems in shared memory multi-processors\cite{hennessy2017computer}. By shifting the task for providing the ordering from bus to directory, the pressure on traffic could get a relaxation.
\par What's more, in practice, the intra-core/processor's cache coherence has a little difference with it under distributed model. 
Table 2 List of Key Literatures about Cache Coherence
Year	Title	Contribution
2003\cite{martin2003token}
Token Coherence: Decoupling Performance and Correctness	proposed a new coherence framework, could separate performance from correctness.
Compare with directory protocol, it has low latency and unordered interconnect, compare with snooping protocol, it avoids indirection. Is one important alternative protocol.
2008\cite{jerger2008virtual}
Virtual Tree Coherence: Leveraging Regions and In-Network Multicast Trees for Scalable Cache Coherence	Using VCT to solve the scalable coherence problem
2008\cite{shacham2008verification}
Verification of chip multiprocessor memory systems using a relaxed scoreboard	Using memory scoreboard to aid simulation-based validation is accurate but complex while implementation sue to its memory dependence and specific implementation dependence. This paper decoupling the specific design with a relaxed scoreboard.
2010\cite{zebchuk2009tagless}
A Tagless Coherence Directory	Using Tagless(TL) as a scalable coherence solution to avoid the area overhead of Directory Protocol in large CMPs.
2010\cite{kelm2010cohesion}
Cohesion: a hybrid memory model for accelerators	This work helps temporal data reassignment and dynamically register memory regions using both hardware and software coherence management. 
2011\cite{vantrease2011atomic}
Atomic coherence: Leveraging nanophotonics to build race-free cache coherence protocols	Using optical mutexes and adding support for speculative coherence. Proposed a new breed of race-free coherence protocol basing on broadcast.
2012\cite{sanchez2012scd}
SCD: A scalable coherence directory with flexible sharer set encoding	Using highly-associative cache to reduce the cost and improve the scalability.

\par Memory Consistency Model
\par The details of memory consistency problem can be found in Hennessy's book\cite{hennessy2017computer} and Adve's tutorial\cite{adve1996shared}. The key factor of Memory Consistency Model is the order of Read/Write. There are many Consistency Models but only there are mainstream in commerce: Sequential Consistency (SC), Total Store Ordering (TSO) Consistency for X86 Architecture\cite{owens2009better} and Relaxed Memory Consistency (RMC) for ARM and Power PC\cite{maranget2012tutorial}.
\par The SC model need both the execution of load and store instruction following the order in program strictly. This kind of consistency could be achieved by using whether a token ring or locking a cache line\cite{lamport1979make}. However, the SC model has limited the optimization in hardware and many compulers\cite{adve1996shared}\cite{boehm2008foundations}.
\par Total Store Ordering Consistency (TSOC) just limit the consistency between memory order and program order of the store instruction. With only a FIFO store buffer based on the SC model could achieve this consistency simply. The store could help increasing the performance of a single load operation and improve the throughput of system.
\par However, above two models have made the system a little more complex. The Relaxed (Weak) Memory Consistency allow all kind of reorder unless the target addresses of two instruction are the same one. The IBM Power pc and ARM are the example of RMC. It can be achieved by replacing the store buffer of TSO to a buffer who can support reorder load/store buffer.
\par In distributed system, the coherence is achieved by transaction with four features: ACID (Atomicity, Consistency, Isolation and Durability).
\par Below are several representative papers about memory consistency.
Year	Title 	Discussion 
1995\cite{guiady1999sc+}
Is sc+ ilp= rc?	Achieving the SC model the same performance as release consistency by providing hardware support.
2007\cite{ceze2007bulksc}
BulkSC: bulk enforcement of sequential consistency	It partitions the execution of program into atomicity, isolation and durability transactions and uses speculation to improve performance. But rely on expensive speculation hardware and cost many to buffer and prevent the speculative stores\cite{ceze2007bulksc}.

2009\cite{blundell2009invisifence}
InvisiFence: performance-transparent memory ordering in conventional multiprocessors	
2011\cite{choi2011denovo}
Rethinking the memory hierarchy for disciplined parallelism. In Parallel Architectures and Compilation Techniques (PACT),	
2012\cite{devietti2012radish}
RADISH: always-on sound and complete Ra D etection in S oftware and H ardware	
2012\cite{singh2013end}
End-To-End Sequential Consistency	Let SC model cost less than TSO by adding an unordered store buffer.

\subsection{Communication for cache coherence}
Due to the complex States and Rules for the cache coherence, there will be numerous communication operations for the transmitting message (such as to make sure there are how many copies of one cache line, how to pass a value to cache directly). The cache coherence technology has become more complex: 
QPI (QuickPath Interconnect) of Intel provides a point to point communication channel between cache clusters and every cache cluster has its own Integrated Memory Controller (IMC) for cache-memory communication\cite{intel2009quickpath}.
The ARM's CoreLink\cite{ARMCoreLinkInterconnect} has several aspect related with cache coherence, for example the CoreLink Generic Interrupt Controller (GIC)\cite{ARMCoreLinkGIC} who makes sure the synchronization interrupt between cores, the CoreLink Cache Coherent Network (CCN)\cite{ARMCoreLinkCCN}  who takes charge of the cache interconnection and cache coherence, and the CoreLink Dynamic Memory Controller (DMC) \cite{ARMCoreLinkMC} who is responsible for the communication between cache and memory. 
\section{Simulation}
\par With the increasing size of the chip, more function and transistors have been built to a computing system. Which has affect the emulation and simulation work in both time and resource cost. According to the report from Wilson in 2014 \cite{Foster2014WRGReport}, the verification work has cost the most of project time. 
\par Meanwhile, there are many verification method and ideas appearing. Simulation Verification, Formal Verification, Hardware Assisted Verification, Emulation and Virtual Prototyping are all used to solve different verification problem. What's more, the System Verilog has met a significate improvement and the merger with Universal Verification Methodology in both Language and Methodology (including the System Verilog Assertion and Function Coverage). More and more tools are playing their roles in verification. The trend of moving the abstraction level from register transfer level RTL to Transaction-level modelling TLM could also help the verification work. 
\par However, there are still many challenges in the pre-silicon verification. Firstly, the increasing number of IPs and the increasing complex of the systems have put forward a serious question for verification: how to make the verification work more efficiently and powerful. Secondly, the trend of software-hardware codesign has also raised one problem, that is how to reduce the gap between pre-silicon design and verification and post-silicon test. 
\par The virtual prototype or in terms of Transaction Level Model (TLM) could help the development start begin from a more abstract level and could also help the hardware design and software design starting in parallel. Start from software-only simulation to hardware-accelerate emulation and FPGA accelerator, the virtual prototype plays important roles in verification work.
\par The TLM 2.0 standard has been regarded as the common abstract level model. SystemC and UVM are two important tools support this standard. The simulation work will also base on TLM by using simulators or virtual prototyping platforms.
\par There are many tools could support the simulation. From MATLAB in algorithm level to ModelSim in HDL and TLM level. Also, the GEM5 support the simulation could be down in a simpler way\cite{binkert2011gem5}. Meanwhile, there are also many works focusing on the speedup of simulation. For example, the ASIM\cite{emer2002asim} by enable the modularity and reusability, the Graphite\cite{miller2010graphite} project and Wisconsin Wind Tunnel II\cite{mukherjee2000wisconsin} who accelerate the simulation on multi-core or using a parallel architecture.
\par Recent years, with the development of GPGPU and the CUDA tools of NVIDIA, the simulation on GPU has also show a new way besides the simulation on CPU\cite{bombieri2012systemc}\cite{nanjundappa2012accelerating}\cite{schmidt2017hybrid}\cite{ventroux2014highly}\cite{vinco2012saga}\cite{weinstock2016systemc}.
\section{Summary and Plan}
\par This chapter is the review of the basic of doing a parallel system. And also shows the trend of IC design. For this time, with the slowdown of the improvement in nanometre fabrication and the perhaps failed of Moore's Law, both the architecture and design ideas will be affect significantly. Meanwhile, both the carbon-based chip and quantum computer may also affect the normal silicon chip. However, in another view, with the development of the personal multimedia devices and many other application, such as edge computing, IoT and cloud computing, the demand of computing resource also be unprecedented huge. The artificial intelligence researchers are aiming to meet the demand in algorithm and software level. Many hardware designers are also join the trend by developing AI chips. The normal processors could also change to another thought, that is using neural network as much as possible to achieving a more powerful and universal many-core system. Using simple and low-cost processors building a massively parallel processor system and implementing neuro parallelism.
\par Based on this thought, the RISC-V could be the simple processor, and system-c and GPU will be used as the simulation platform. The SpiNNaker will be studied as an important case study who achieving a success Spiking Neural Network on Massively Parallel Processors. Then I will start a practice by making one simple processor on RV32-I in order to understand the knowledge I have reviewed above.
\par What's more, there are still many factors not covered in this chapter, for example the power management and low-cost design, the programming and software for parallel system. These sections will be studied in next month.
